# -*- coding: utf-8 -*-
"""
ä» quotsoft.net ä¸‹è½½å†å²ç©ºæ°”è´¨é‡æ•°æ®
æ•°æ®æ ¼å¼: china_cities_YYYYMMDD.csv
"""

import os
import sys
import time
import requests
import pandas as pd
from datetime import datetime, timedelta
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent.parent))
from src.config import RAW_DATA_DIR, DATA_COLLECTION_CONFIG, YANGZHOU_CONFIG

# å†å² AQI æ•°æ®ç›®å½•
HISTORICAL_AQI_DIR = RAW_DATA_DIR / "historical_aqi"

# quotsoft.net åŸºç¡€ URL
BASE_URL = "https://quotsoft.net/air/data/china_cities_{date}.csv"


def download_single_day(date_str):
    """
    ä¸‹è½½å•æ—¥æ•°æ®

    Args:
        date_str: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œæ ¼å¼ YYYYMMDD

    Returns:
        (date_str, success, message)
    """
    url = BASE_URL.format(date=date_str)
    output_file = HISTORICAL_AQI_DIR / f"china_cities_{date_str}.csv"

    # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ä¸”å¤§å°åˆç†ï¼Œè·³è¿‡
    if output_file.exists() and output_file.stat().st_size > 1000:
        return (date_str, True, "å·²å­˜åœ¨")

    try:
        response = requests.get(url, timeout=30)
        if response.status_code == 200:
            # ä¿å­˜åŸå§‹æ•°æ®
            with open(output_file, 'wb') as f:
                f.write(response.content)
            return (date_str, True, "ä¸‹è½½æˆåŠŸ")
        else:
            return (date_str, False, f"HTTP {response.status_code}")
    except Exception as e:
        return (date_str, False, str(e))


def download_date_range(start_date, end_date, max_workers=5):
    """
    ä¸‹è½½æ—¥æœŸèŒƒå›´å†…çš„æ‰€æœ‰æ•°æ®

    Args:
        start_date: å¼€å§‹æ—¥æœŸï¼Œæ ¼å¼ YYYY-MM-DD
        end_date: ç»“æŸæ—¥æœŸï¼Œæ ¼å¼ YYYY-MM-DD
        max_workers: å¹¶å‘ä¸‹è½½æ•°
    """
    start = datetime.strptime(start_date, "%Y-%m-%d")
    end = datetime.strptime(end_date, "%Y-%m-%d")

    # ç”Ÿæˆæ—¥æœŸåˆ—è¡¨
    dates = []
    current = start
    while current <= end:
        dates.append(current.strftime("%Y%m%d"))
        current += timedelta(days=1)

    print(f"ğŸ“¥ å‡†å¤‡ä¸‹è½½ {len(dates)} å¤©çš„æ•°æ®...")
    print(f"   æ—¶é—´èŒƒå›´: {start_date} ~ {end_date}")

    success_count = 0
    fail_count = 0
    skip_count = 0

    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘ä¸‹è½½
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {executor.submit(download_single_day, d): d for d in dates}

        for i, future in enumerate(as_completed(futures)):
            date_str, success, message = future.result()
            if success:
                if message == "å·²å­˜åœ¨":
                    skip_count += 1
                else:
                    success_count += 1
            else:
                fail_count += 1
                print(f"   âŒ {date_str}: {message}")

            # æ¯ 100 ä¸ªæ˜¾ç¤ºè¿›åº¦
            if (i + 1) % 100 == 0:
                print(f"   è¿›åº¦: {i+1}/{len(dates)}")

    print(f"\nâœ… ä¸‹è½½å®Œæˆ!")
    print(f"   æˆåŠŸ: {success_count}, è·³è¿‡: {skip_count}, å¤±è´¥: {fail_count}")


def extract_yangzhou_data():
    """
    ä»æ‰€æœ‰æ—¥æ–‡ä»¶ä¸­æå–æ‰¬å·æ•°æ®ï¼Œåˆå¹¶æˆå•ä¸ªæ–‡ä»¶
    """
    print("\nğŸ“Š æå–æ‰¬å·æ•°æ®...")

    all_files = sorted(HISTORICAL_AQI_DIR.glob("china_cities_*.csv"))
    print(f"   æ‰¾åˆ° {len(all_files)} ä¸ªæ•°æ®æ–‡ä»¶")

    all_data = []

    for i, file_path in enumerate(all_files):
        try:
            df = pd.read_csv(file_path, encoding='utf-8')

            # æ£€æŸ¥æ‰¬å·æ˜¯å¦åœ¨åˆ—ä¸­
            if 'æ‰¬å·' not in df.columns:
                continue

            # æå–æ‰¬å·æ•°æ®
            yangzhou_df = df[['date', 'hour', 'type', 'æ‰¬å·']].copy()
            yangzhou_df = yangzhou_df.rename(columns={'æ‰¬å·': 'value'})

            all_data.append(yangzhou_df)

        except Exception as e:
            print(f"   âš ï¸ è¯»å–å¤±è´¥: {file_path.name} - {e}")

        if (i + 1) % 100 == 0:
            print(f"   è¿›åº¦: {i+1}/{len(all_files)}")

    if not all_data:
        print("   âŒ æ²¡æœ‰æ‰¾åˆ°æ‰¬å·æ•°æ®")
        return None

    # åˆå¹¶æ‰€æœ‰æ•°æ®
    combined_df = pd.concat(all_data, ignore_index=True)

    # è½¬æ¢ä¸ºå®½æ ¼å¼ï¼ˆæ¯ä¸ªæ±¡æŸ“ç‰©ä¸€åˆ—ï¼‰
    pivot_df = combined_df.pivot_table(
        index=['date', 'hour'],
        columns='type',
        values='value',
        aggfunc='first'
    ).reset_index()

    # é‡å‘½ååˆ—
    pivot_df.columns.name = None

    # åˆ›å»º datetime åˆ—
    pivot_df['datetime'] = pd.to_datetime(
        pivot_df['date'].astype(str) + ' ' + pivot_df['hour'].astype(str).str.zfill(2) + ':00:00'
    )

    # æ’åº
    pivot_df = pivot_df.sort_values('datetime').reset_index(drop=True)

    # ä¿å­˜
    output_file = RAW_DATA_DIR / "yangzhou_aqi_historical.csv"
    pivot_df.to_csv(output_file, index=False, encoding='utf-8')

    print(f"\nâœ… æ‰¬å·å†å² AQI æ•°æ®å·²ä¿å­˜: {output_file}")
    print(f"   è®°å½•æ•°: {len(pivot_df)}")
    print(f"   æ—¶é—´èŒƒå›´: {pivot_df['datetime'].min()} ~ {pivot_df['datetime'].max()}")
    print(f"   åˆ—: {list(pivot_df.columns)}")

    return pivot_df


def extract_upwind_cities_data():
    """
    æå–ä¸Šé£å‘åŸå¸‚çš„ AQI æ•°æ®ï¼ˆç”¨äºè·¨åŒºåŸŸæ±¡æŸ“ä¼ è¾“ç‰¹å¾ï¼‰
    """
    print("\nğŸ“Š æå–ä¸Šé£å‘åŸå¸‚æ•°æ®...")

    upwind_cities = list(YANGZHOU_CONFIG['upwind_cities'].keys())
    print(f"   ä¸Šé£å‘åŸå¸‚: {upwind_cities}")

    all_files = sorted(HISTORICAL_AQI_DIR.glob("china_cities_*.csv"))

    all_data = []

    for i, file_path in enumerate(all_files):
        try:
            df = pd.read_csv(file_path, encoding='utf-8')

            # æ£€æŸ¥åŸå¸‚æ˜¯å¦åœ¨åˆ—ä¸­
            available_cities = [c for c in upwind_cities if c in df.columns]
            if not available_cities:
                continue

            # æå–æ•°æ®
            cols = ['date', 'hour', 'type'] + available_cities
            city_df = df[cols].copy()

            all_data.append(city_df)

        except Exception as e:
            pass

        if (i + 1) % 100 == 0:
            print(f"   è¿›åº¦: {i+1}/{len(all_files)}")

    if not all_data:
        print("   âŒ æ²¡æœ‰æ‰¾åˆ°ä¸Šé£å‘åŸå¸‚æ•°æ®")
        return None

    # åˆå¹¶
    combined_df = pd.concat(all_data, ignore_index=True)

    # ä¿å­˜å®½æ ¼å¼æ•°æ®ï¼ˆæ¯ä¸ªåŸå¸‚çš„æ¯ä¸ªæŒ‡æ ‡ä¸€åˆ—ï¼‰
    result_dfs = []

    for city in upwind_cities:
        if city in combined_df.columns:
            city_data = combined_df[['date', 'hour', 'type', city]].copy()
            city_pivot = city_data.pivot_table(
                index=['date', 'hour'],
                columns='type',
                values=city,
                aggfunc='first'
            ).reset_index()
            city_pivot.columns.name = None

            # é‡å‘½ååˆ—ï¼Œæ·»åŠ åŸå¸‚å‰ç¼€
            rename_dict = {col: f"{city}_{col}" for col in city_pivot.columns
                           if col not in ['date', 'hour']}
            city_pivot = city_pivot.rename(columns=rename_dict)

            result_dfs.append(city_pivot)

    if not result_dfs:
        return None

    # åˆå¹¶æ‰€æœ‰åŸå¸‚æ•°æ®
    merged_df = result_dfs[0]
    for df in result_dfs[1:]:
        merged_df = merged_df.merge(df, on=['date', 'hour'], how='outer')

    # åˆ›å»º datetime åˆ—
    merged_df['datetime'] = pd.to_datetime(
        merged_df['date'].astype(str) + ' ' + merged_df['hour'].astype(str).str.zfill(2) + ':00:00'
    )

    # æ’åº
    merged_df = merged_df.sort_values('datetime').reset_index(drop=True)

    # ä¿å­˜
    output_file = RAW_DATA_DIR / "upwind_cities_aqi_historical.csv"
    merged_df.to_csv(output_file, index=False, encoding='utf-8')

    print(f"\nâœ… ä¸Šé£å‘åŸå¸‚å†å² AQI æ•°æ®å·²ä¿å­˜: {output_file}")
    print(f"   è®°å½•æ•°: {len(merged_df)}")

    return merged_df


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("  æ‰¬å·ç©ºæ°”è´¨é‡å†å²æ•°æ®ä¸‹è½½")
    print("=" * 60)

    # è·å–é…ç½®
    start_date = DATA_COLLECTION_CONFIG["historical_start_date"]
    end_date = DATA_COLLECTION_CONFIG["historical_end_date"]

    # 1. ä¸‹è½½å†å²æ•°æ®
    download_date_range(start_date, end_date, max_workers=10)

    # 2. æå–æ‰¬å·æ•°æ®
    extract_yangzhou_data()

    # 3. æå–ä¸Šé£å‘åŸå¸‚æ•°æ®
    extract_upwind_cities_data()


if __name__ == "__main__":
    main()
