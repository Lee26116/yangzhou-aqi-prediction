# -*- coding: utf-8 -*-
"""
æ¨¡å‹è§£é‡Šæ¨¡å—
ä½¿ç”¨ SHAP è§£é‡Šæ¨¡å‹é¢„æµ‹
"""

import sys
import json
import pickle
import numpy as np
import pandas as pd
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent.parent))
from src.config import FEATURES_DATA_DIR, MODELS_DIR, DOCS_DIR, DASHBOARD_DATA_DIR

# å°è¯•å¯¼å…¥ SHAP
try:
    import shap
    HAS_SHAP = True
except ImportError:
    HAS_SHAP = False
    print("âš ï¸ SHAP æœªå®‰è£…ï¼Œæ¨¡å‹è§£é‡ŠåŠŸèƒ½å°†ä¸å¯ç”¨")


def load_model():
    """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
    model_path = MODELS_DIR / "xgboost_model.pkl"

    if not model_path.exists():
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        return None

    with open(model_path, 'rb') as f:
        model = pickle.load(f)

    return model


def _patch_shap_xgboost_compat():
    """ä¿®å¤ XGBoost 3.x + SHAP å…¼å®¹æ€§é—®é¢˜
    XGBoost 3.x å°† base_score å­˜ä¸º '[5.97E1]' æ ¼å¼ï¼ˆUBJSONï¼‰ï¼ŒSHAP float() æ— æ³•è§£æ
    é€šè¿‡ monkey-patch XGBTreeModelLoader.__init__ åœ¨ UBJSON è§£ç åä¿®å¤"""
    try:
        import shap.explainers._tree as tree_mod
        _orig_init = tree_mod.XGBTreeModelLoader.__init__

        if getattr(tree_mod.XGBTreeModelLoader, '_patched', False):
            return

        # ä¿å­˜åŸå§‹ decode_ubjson_buffer
        _orig_decode = tree_mod.decode_ubjson_buffer

        def _fix_bracket_values(obj):
            """é€’å½’ä¿®å¤ dict ä¸­ '[xxx]' æ ¼å¼çš„å­—ç¬¦ä¸²å€¼"""
            if isinstance(obj, dict):
                for k, v in obj.items():
                    if isinstance(v, str) and v.startswith('[') and v.endswith(']'):
                        try:
                            obj[k] = str(float(v.strip('[]')))
                        except ValueError:
                            pass
                    elif isinstance(v, (dict, list)):
                        _fix_bracket_values(v)
            elif isinstance(obj, list):
                for item in obj:
                    if isinstance(item, (dict, list)):
                        _fix_bracket_values(item)

        def _patched_decode(fd):
            result = _orig_decode(fd)
            _fix_bracket_values(result)
            return result

        tree_mod.decode_ubjson_buffer = _patched_decode
        tree_mod.XGBTreeModelLoader._patched = True
        print("   ğŸ”§ å·²ä¿®å¤ XGBoost 3.x / SHAP å…¼å®¹æ€§")
    except Exception as e:
        print(f"   âš ï¸ SHAP å…¼å®¹æ€§ä¿®å¤å¤±è´¥: {e}")


def compute_shap_values(model, X, max_samples=1000):
    """
    è®¡ç®— SHAP å€¼

    Args:
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        X: ç‰¹å¾æ•°æ®
        max_samples: æœ€å¤§æ ·æœ¬æ•°ï¼ˆç”¨äºåŠ é€Ÿï¼‰

    Returns:
        shap.Explanation: SHAP å€¼
    """
    if not HAS_SHAP:
        print("âŒ SHAP æœªå®‰è£…")
        return None

    print("ğŸ” è®¡ç®— SHAP å€¼...")

    # ä¿®å¤ XGBoost 3.x + SHAP å…¼å®¹æ€§
    _patch_shap_xgboost_compat()

    # å¦‚æœæ ·æœ¬å¤ªå¤šï¼Œéšæœºé‡‡æ ·
    if len(X) > max_samples:
        X_sample = X.sample(n=max_samples, random_state=42)
    else:
        X_sample = X

    # åˆ›å»º SHAP è§£é‡Šå™¨
    explainer = shap.TreeExplainer(model)
    shap_values = explainer(X_sample)

    print(f"   å®Œæˆ! æ ·æœ¬æ•°: {len(X_sample)}")

    return shap_values, X_sample


def get_global_feature_importance(shap_values):
    """
    è·å–å…¨å±€ç‰¹å¾é‡è¦æ€§

    Args:
        shap_values: SHAP å€¼

    Returns:
        DataFrame: ç‰¹å¾é‡è¦æ€§æ’åº
    """
    importance = np.abs(shap_values.values).mean(axis=0)
    importance_df = pd.DataFrame({
        'feature': shap_values.feature_names,
        'importance': importance
    }).sort_values('importance', ascending=False)

    return importance_df


def explain_single_prediction(model, X, index, feature_names):
    """
    è§£é‡Šå•ä¸ªé¢„æµ‹

    Args:
        model: æ¨¡å‹
        X: ç‰¹å¾æ•°æ®
        index: è¦è§£é‡Šçš„æ ·æœ¬ç´¢å¼•
        feature_names: ç‰¹å¾ååˆ—è¡¨

    Returns:
        dict: è§£é‡Šç»“æœ
    """
    if not HAS_SHAP:
        return None

    explainer = shap.TreeExplainer(model)

    if isinstance(X, pd.DataFrame):
        sample = X.iloc[[index]]
    else:
        sample = X[index:index+1]

    shap_values = explainer(sample)

    # è·å–æ¯ä¸ªç‰¹å¾çš„è´¡çŒ®
    contributions = []
    for i, (feat, val, shap_val) in enumerate(zip(feature_names, sample.values[0], shap_values.values[0])):
        contributions.append({
            'feature': feat,
            'value': float(val) if not np.isnan(val) else None,
            'shap_value': float(shap_val)
        })

    # æŒ‰ SHAP å€¼ç»å¯¹å€¼æ’åº
    contributions.sort(key=lambda x: abs(x['shap_value']), reverse=True)

    return {
        'prediction': float(model.predict(sample)[0]),
        'base_value': float(shap_values.base_values[0]),
        'contributions': contributions[:20]  # åªè¿”å› Top 20
    }


def generate_shap_summary_data(shap_values, X_sample, top_n=20):
    """
    ç”Ÿæˆ SHAP Summary Plot æ‰€éœ€çš„æ•°æ®

    Args:
        shap_values: SHAP å€¼
        X_sample: æ ·æœ¬æ•°æ®
        top_n: æ˜¾ç¤ºçš„ç‰¹å¾æ•°é‡

    Returns:
        dict: å¯ç”¨äºå‰ç«¯å¯è§†åŒ–çš„æ•°æ®
    """
    # è·å–æœ€é‡è¦çš„ç‰¹å¾
    importance = np.abs(shap_values.values).mean(axis=0)
    top_indices = np.argsort(importance)[-top_n:][::-1]

    feature_names = [shap_values.feature_names[i] for i in top_indices]

    # å‡†å¤‡æ•°æ®
    summary_data = []
    for i, feat_idx in enumerate(top_indices):
        feat_name = shap_values.feature_names[feat_idx]

        # è·å–è¯¥ç‰¹å¾çš„ SHAP å€¼å’ŒåŸå§‹å€¼
        shap_vals = shap_values.values[:, feat_idx]
        feat_vals = X_sample.iloc[:, feat_idx].values if isinstance(X_sample, pd.DataFrame) else X_sample[:, feat_idx]

        # å½’ä¸€åŒ–ç‰¹å¾å€¼ç”¨äºç€è‰²
        feat_min = np.nanmin(feat_vals)
        feat_max = np.nanmax(feat_vals)
        if feat_max > feat_min:
            feat_normalized = (feat_vals - feat_min) / (feat_max - feat_min)
        else:
            feat_normalized = np.zeros_like(feat_vals)

        summary_data.append({
            'feature': feat_name,
            'importance': float(importance[feat_idx]),
            'shap_values': shap_vals.tolist(),
            'feature_values': feat_vals.tolist(),
            'feature_normalized': feat_normalized.tolist()
        })

    return {
        'features': feature_names,
        'data': summary_data
    }


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("  æ¨¡å‹è§£é‡Š (SHAP)")
    print("=" * 60)

    if not HAS_SHAP:
        print("âŒ SHAP æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install shap")
        return

    # åŠ è½½æ¨¡å‹
    model = load_model()
    if model is None:
        return

    # è¯»å–ç‰¹å¾æ•°æ®
    input_file = FEATURES_DATA_DIR / "yangzhou_features_selected.csv"
    if not input_file.exists():
        input_file = FEATURES_DATA_DIR / "yangzhou_features.csv"

    if not input_file.exists():
        print(f"âŒ ç‰¹å¾æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
        return

    df = pd.read_csv(input_file)
    df['datetime'] = pd.to_datetime(df['datetime'])

    # å‡†å¤‡ç‰¹å¾
    feature_cols = [c for c in df.columns if c not in ['datetime', 'AQI', 'AQI_target', 'date']]
    X = df[feature_cols].replace([np.inf, -np.inf], np.nan).dropna()

    print(f"ğŸ“– è¯»å–æ•°æ®: {len(X)} è¡Œ, {len(feature_cols)} ä¸ªç‰¹å¾")

    # è®¡ç®— SHAP å€¼
    shap_values, X_sample = compute_shap_values(model, X)

    if shap_values is None:
        return

    # è·å–å…¨å±€ç‰¹å¾é‡è¦æ€§
    importance_df = get_global_feature_importance(shap_values)
    print("\nğŸ“Š Top 15 ç‰¹å¾é‡è¦æ€§ (SHAP):")
    for i, row in importance_df.head(15).iterrows():
        print(f"   {i+1:2d}. {row['feature']:40s} {row['importance']:.4f}")

    # ä¿å­˜ SHAP å€¼
    shap_file = MODELS_DIR / "shap_values.pkl"
    with open(shap_file, 'wb') as f:
        pickle.dump({
            'shap_values': shap_values,
            'X_sample': X_sample,
            'feature_names': feature_cols
        }, f)
    print(f"\nâœ… SHAP å€¼å·²ä¿å­˜: {shap_file}")

    # ç”Ÿæˆ Dashboard æ•°æ®
    summary_data = generate_shap_summary_data(shap_values, X_sample)

    dashboard_file = DASHBOARD_DATA_DIR / "shap_summary.json"
    with open(dashboard_file, 'w', encoding='utf-8') as f:
        json.dump(summary_data, f, ensure_ascii=False)

    print(f"âœ… Dashboard SHAP æ•°æ®å·²ä¿å­˜: {dashboard_file}")

    # ä¿å­˜ç‰¹å¾é‡è¦æ€§ï¼ˆSHAPç‰ˆæœ¬ï¼‰
    importance_file = DOCS_DIR / "shap_feature_importance.json"
    importance_df.to_json(importance_file, orient='records', force_ascii=False, indent=2)
    print(f"âœ… SHAP ç‰¹å¾é‡è¦æ€§å·²ä¿å­˜: {importance_file}")

    return shap_values


if __name__ == "__main__":
    main()
