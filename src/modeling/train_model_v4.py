# -*- coding: utf-8 -*-
"""
æ¨¡å‹è®­ç»ƒ v4 - å¯è§£é‡Šæ€§ä¼˜å…ˆ

åŸåˆ™ï¼š
1. ä¿ç•™æ‰€æœ‰æœ‰ç‰©ç†æ„ä¹‰çš„ç‰¹å¾ï¼Œä¸å› ç»Ÿè®¡é‡è¦æ€§ä½è€Œåˆ é™¤
2. åˆ†ææ¨¡å‹å­¦åˆ°çš„å…³ç³»æ˜¯å¦ç¬¦åˆç‰©ç†é¢„æœŸ
3. ç”Ÿæˆå¯è§£é‡Šæ€§æŠ¥å‘Šï¼Œè§£é‡Šæ¯ä¸ªç‰¹å¾å¯¹é¢„æµ‹çš„è´¡çŒ®
"""

import sys
import pickle
import json
import numpy as np
import pandas as pd
from pathlib import Path
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from xgboost import XGBRegressor

sys.path.insert(0, str(Path(__file__).parent.parent.parent))
from src.config import FEATURES_DATA_DIR, MODELS_DIR, DASHBOARD_DATA_DIR


# ç‰¹å¾çš„ç‰©ç†æ„ä¹‰ï¼ˆç”¨äºå¯è§£é‡Šæ€§æŠ¥å‘Šï¼‰
FEATURE_INTERPRETATIONS = {
    'AQI_lag_1h': {
        'name': '1hå‰AQI',
        'expected': '+',
        'interpretation': '1å°æ—¶å‰çš„AQIå€¼ï¼Œç©ºæ°”è´¨é‡å…·æœ‰æ—¶é—´æƒ¯æ€§'
    },
    'AQI_lag_6h': {
        'name': '6hå‰AQI',
        'expected': '+',
        'interpretation': '6å°æ—¶å‰çš„AQIå€¼ï¼Œç”¨äº6hé¢„æµ‹'
    },
    'AQI_lag_12h': {
        'name': '12hå‰AQI',
        'expected': '+',
        'interpretation': '12å°æ—¶å‰çš„AQIå€¼ï¼Œç”¨äº12hé¢„æµ‹'
    },
    'AQI_lag_24h': {
        'name': '24hå‰AQI',
        'expected': '+',
        'interpretation': '24å°æ—¶å‰çš„AQIå€¼ï¼Œç”¨äº24hé¢„æµ‹'
    },
    'AQI_yesterday_same_hour': {
        'name': 'æ˜¨æ—¥åŒæœŸAQI',
        'expected': '+',
        'interpretation': 'ç©ºæ°”è´¨é‡å­˜åœ¨æ—¥å‘¨æœŸæ€§ï¼Œæ˜¨å¤©åŒä¸€æ—¶åˆ»çš„æƒ…å†µæœ‰å‚è€ƒä»·å€¼'
    },
    'AQI_rolling_mean_24h': {
        'name': '24å°æ—¶å¹³å‡AQI',
        'expected': '+',
        'interpretation': 'åæ˜ è¿‘æœŸæ•´ä½“æ±¡æŸ“æ°´å¹³ï¼Œæ¶ˆé™¤çŸ­æœŸæ³¢åŠ¨'
    },
    'AQI_trend_24h': {
        'name': 'AQIå˜åŒ–è¶‹åŠ¿',
        'expected': '+',
        'interpretation': 'æ­£å€¼è¡¨ç¤ºæ±¡æŸ“åŠ é‡è¶‹åŠ¿ï¼Œè´Ÿå€¼è¡¨ç¤ºæ”¹å–„è¶‹åŠ¿'
    },
    'PM2.5_current': {
        'name': 'PM2.5æµ“åº¦',
        'expected': '+',
        'interpretation': 'PM2.5æ˜¯AQIçš„ä¸»è¦ç»„æˆéƒ¨åˆ†ï¼Œç›´æ¥å½±å“ç©ºæ°”è´¨é‡ç­‰çº§'
    },
    'PM10_current': {
        'name': 'PM10æµ“åº¦',
        'expected': '+',
        'interpretation': 'PM10åŒ…å«ç²—é¢—ç²’ç‰©ï¼Œæ˜¯AQIçš„é‡è¦ç»„æˆéƒ¨åˆ†'
    },
    'wind_speed': {
        'name': 'é£é€Ÿ',
        'expected': '-',
        'interpretation': 'é«˜é£é€Ÿä¿ƒè¿›æ±¡æŸ“ç‰©æ‰©æ•£ï¼Œé™ä½AQIï¼ˆè´Ÿç›¸å…³ï¼‰'
    },
    'is_low_wind': {
        'name': 'ä½é£é€Ÿæ ‡è®°',
        'expected': '+',
        'interpretation': 'é£é€Ÿ<2m/sæ—¶å¤§æ°”æ‰©æ•£èƒ½åŠ›å¼±ï¼Œæ±¡æŸ“ç‰©å®¹æ˜“ç´¯ç§¯'
    },
    'pressure': {
        'name': 'æ°”å‹',
        'expected': '+',
        'interpretation': 'é«˜æ°”å‹é€šå¸¸ä¼´éšä¸‹æ²‰æ°”æµå’Œç¨³å®šå±‚ç»“ï¼Œä¸åˆ©äºæ‰©æ•£'
    },
    'pressure_change_24h': {
        'name': 'æ°”å‹å˜åŒ–',
        'expected': '-',
        'interpretation': 'æ°”å‹ä¸‹é™é€šå¸¸æ„å‘³ç€å¤©æ°”ç³»ç»Ÿè¿‡å¢ƒï¼Œæœ‰åˆ©äºæ±¡æŸ“ç‰©æ¸…é™¤'
    },
    'temperature': {
        'name': 'æ¸©åº¦',
        'expected': '+/-',
        'interpretation': 'æ¸©åº¦å½±å“å¤æ‚ï¼šé«˜æ¸©åŠ é€Ÿå…‰åŒ–å­¦ååº”ï¼Œä½†ä¹Ÿä¿ƒè¿›å¯¹æµ'
    },
    'temp_daily_range': {
        'name': 'æ—¥æ¸©å·®',
        'expected': '+',  # æ ¹æ®å®é™…æ•°æ®æ›´æ–°
        'interpretation': 'æ‰¬å·å®é™…è§„å¾‹ï¼šæ—¥æ¸©å·®å¤§=æ™´æœ—å¤©æ°”=é«˜å‹æ§åˆ¶=ä¸åˆ©æ‰©æ•£'
    },
    'humidity': {
        'name': 'ç›¸å¯¹æ¹¿åº¦',
        'expected': '-',  # æ ¹æ®å®é™…æ•°æ®æ›´æ–°
        'interpretation': 'æ‰¬å·å®é™…è§„å¾‹ï¼šé«˜æ¹¿åº¦é€šå¸¸ä¼´éšé™é›¨/æµ·æ´‹æ°”å›¢ï¼Œæœ‰åˆ©äºæ±¡æŸ“ç‰©æ¸…é™¤'
    },
    'is_high_humidity': {
        'name': 'é«˜æ¹¿åº¦æ ‡è®°',
        'expected': '-',  # æ ¹æ®å®é™…æ•°æ®æ›´æ–°
        'interpretation': 'æ¹¿åº¦>80%æ—¶é€šå¸¸ä¼´éšé™é›¨ï¼Œå†²åˆ·æ±¡æŸ“ç‰©'
    },
    'dew_point': {
        'name': 'éœ²ç‚¹æ¸©åº¦',
        'expected': '-',  # æ ¹æ®å®é™…æ•°æ®æ›´æ–°
        'interpretation': 'é«˜éœ²ç‚¹åæ˜ å¤§æ°”æ¹¿æ¶¦ï¼Œé€šå¸¸ä¼´éšé™æ°´å¤©æ°”'
    },
    'is_rush_hour': {
        'name': 'æ—©æ™šé«˜å³°',
        'expected': '+',
        'interpretation': '7-9ç‚¹å’Œ17-19ç‚¹äº¤é€šæ’æ”¾å¢åŠ '
    },
    'is_weekend': {
        'name': 'å‘¨æœ«',
        'expected': '-',
        'interpretation': 'å‘¨æœ«å·¥ä¸šå’Œäº¤é€šæ´»åŠ¨å‡å°‘ï¼Œæ’æ”¾é™ä½'
    },
    'is_heating_season': {
        'name': 'ä¾›æš–å­£',
        'expected': '+',
        'interpretation': '11æœˆè‡³3æœˆä¸ºä¾›æš–å­£ï¼Œç‡ƒç…¤æ’æ”¾å¢åŠ '
    },
    'hour_sin': {
        'name': 'å°æ—¶å‘¨æœŸ(sin)',
        'expected': 'å‘¨æœŸ',
        'interpretation': 'æ•æ‰ä¸€å¤©ä¸­çš„å‘¨æœŸæ€§å˜åŒ–'
    },
    'hour_cos': {
        'name': 'å°æ—¶å‘¨æœŸ(cos)',
        'expected': 'å‘¨æœŸ',
        'interpretation': 'ä¸hour_siné…åˆå®Œæ•´è¡¨ç¤º24å°æ—¶å‘¨æœŸ'
    },
    'month_sin': {
        'name': 'æœˆä»½å‘¨æœŸ(sin)',
        'expected': 'å‘¨æœŸ',
        'interpretation': 'æ•æ‰å­£èŠ‚æ€§å˜åŒ–'
    },
    'month_cos': {
        'name': 'æœˆä»½å‘¨æœŸ(cos)',
        'expected': 'å‘¨æœŸ',
        'interpretation': 'ä¸month_siné…åˆå®Œæ•´è¡¨ç¤ºå¹´å‘¨æœŸ'
    },
}


def load_data(horizon_hours):
    """åŠ è½½ç‰¹å¾æ•°æ®"""
    input_file = FEATURES_DATA_DIR / f"yangzhou_features_v4_{horizon_hours}h.csv"

    if not input_file.exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
        return None, None, None

    df = pd.read_csv(input_file)
    df['datetime'] = pd.to_datetime(df['datetime'])
    df = df.sort_values('datetime').reset_index(drop=True)

    # åˆ†ç¦»ç‰¹å¾å’Œç›®æ ‡
    feature_cols = [c for c in df.columns if c not in ['datetime', 'AQI_target']]
    X = df[feature_cols].fillna(df[feature_cols].median())
    y = df['AQI_target']

    return X, y, feature_cols


def train_and_evaluate(X, y, feature_cols, horizon_hours):
    """è®­ç»ƒæ¨¡å‹å¹¶è¯„ä¼°"""
    print(f"\nğŸ¤– è®­ç»ƒ XGBoost æ¨¡å‹ ({horizon_hours}h é¢„æµ‹)...")

    # æ—¶åºäº¤å‰éªŒè¯
    tscv = TimeSeriesSplit(n_splits=5)
    cv_results = {'MAE': [], 'RMSE': [], 'R2': []}

    params = {
        'n_estimators': 150,
        'max_depth': 5,
        'learning_rate': 0.1,
        'subsample': 0.8,
        'colsample_bytree': 0.8,
        'random_state': 42,
        'n_jobs': -1
    }

    for fold, (train_idx, test_idx) in enumerate(tscv.split(X)):
        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]

        model = XGBRegressor(**params)
        model.fit(X_train, y_train, verbose=False)
        y_pred = model.predict(X_test)

        cv_results['MAE'].append(mean_absolute_error(y_test, y_pred))
        cv_results['RMSE'].append(np.sqrt(mean_squared_error(y_test, y_pred)))
        cv_results['R2'].append(r2_score(y_test, y_pred))

    print(f"   MAE:  {np.mean(cv_results['MAE']):.2f} Â± {np.std(cv_results['MAE']):.2f}")
    print(f"   RMSE: {np.mean(cv_results['RMSE']):.2f} Â± {np.std(cv_results['RMSE']):.2f}")
    print(f"   RÂ²:   {np.mean(cv_results['R2']):.4f} Â± {np.std(cv_results['R2']):.4f}")

    # è®­ç»ƒæœ€ç»ˆæ¨¡å‹
    final_model = XGBRegressor(**params)
    final_model.fit(X, y, verbose=False)

    return final_model, cv_results


def generate_interpretability_report(model, feature_cols, X, y, horizon_hours):
    """ç”Ÿæˆå¯è§£é‡Šæ€§æŠ¥å‘Š"""
    print(f"\nğŸ“Š ç”Ÿæˆå¯è§£é‡Šæ€§æŠ¥å‘Š...")

    # ç‰¹å¾é‡è¦æ€§
    importance = model.feature_importances_
    imp_df = pd.DataFrame({
        'feature': feature_cols,
        'importance': importance
    }).sort_values('importance', ascending=False)

    # ç‰¹å¾ä¸ç›®æ ‡çš„ç›¸å…³æ€§
    correlations = X.corrwith(y)

    # æ„å»ºå¯è§£é‡Šæ€§æŠ¥å‘Š
    report = {
        'horizon_hours': horizon_hours,
        'features': []
    }

    print(f"\n   {'ç‰¹å¾':<25} {'é‡è¦æ€§':>8} {'ç›¸å…³æ€§':>8} {'æ–¹å‘':>6} {'ç‰©ç†æ„ä¹‰'}")
    print("   " + "-" * 90)

    for _, row in imp_df.iterrows():
        feat = row['feature']
        imp = row['importance']
        corr = correlations.get(feat, 0)

        # è·å–ç‰¹å¾è§£é‡Š
        interp = FEATURE_INTERPRETATIONS.get(feat, {})
        direction = '+' if corr > 0 else '-' if corr < 0 else '~0'

        feature_info = {
            'name': feat,
            'display_name': interp.get('name', feat),
            'importance': float(imp),
            'correlation': float(corr),
            'direction': direction,
            'interpretation': interp.get('interpretation', ''),
            'category': get_feature_category(feat)
        }
        report['features'].append(feature_info)

        # æ‰“å°å‰15ä¸ªç‰¹å¾
        if imp > 0.01 or len(report['features']) <= 15:
            meaning = interp.get('interpretation', '')[:40]
            print(f"   {feat:<25} {imp:>7.1%} {corr:>+7.3f}  {direction:>4}   {meaning}...")

    return report


def get_feature_category(feat):
    """è·å–ç‰¹å¾ç±»åˆ«"""
    if 'AQI' in feat:
        return 'AQIå†å²'
    elif 'PM' in feat:
        return 'æ±¡æŸ“ç‰©'
    elif feat in ['wind_speed', 'is_low_wind', 'pressure', 'pressure_change_24h', 'temp_daily_range']:
        return 'æ°”è±¡-æ‰©æ•£'
    elif feat in ['temperature', 'humidity', 'is_high_humidity', 'dew_point']:
        return 'æ°”è±¡-åŒ–å­¦'
    elif feat in ['is_rush_hour', 'is_weekend', 'is_heating_season']:
        return 'æ—¶é—´-æ’æ”¾'
    elif 'sin' in feat or 'cos' in feat:
        return 'æ—¶é—´-å‘¨æœŸ'
    else:
        return 'å…¶ä»–'


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 70)
    print("  æ¨¡å‹è®­ç»ƒ v4 - å¯è§£é‡Šæ€§ä¼˜å…ˆ")
    print("=" * 70)

    horizons = [1, 6, 12, 24]
    all_reports = {}

    for h in horizons:
        print(f"\n{'='*70}")
        print(f"  é¢„æµ‹æ—¶é—´è·¨åº¦: {h} å°æ—¶")
        print(f"{'='*70}")

        # åŠ è½½æ•°æ®
        X, y, feature_cols = load_data(h)
        if X is None:
            continue

        print(f"\nğŸ“‚ æ•°æ®: {len(X)} æ ·æœ¬, {len(feature_cols)} ç‰¹å¾")

        # è®­ç»ƒæ¨¡å‹
        model, cv_results = train_and_evaluate(X, y, feature_cols, h)

        # ç”Ÿæˆå¯è§£é‡Šæ€§æŠ¥å‘Š
        report = generate_interpretability_report(model, feature_cols, X, y, h)
        report['metrics'] = {
            'MAE': float(np.mean(cv_results['MAE'])),
            'RMSE': float(np.mean(cv_results['RMSE'])),
            'R2': float(np.mean(cv_results['R2']))
        }
        all_reports[h] = report

        # ä¿å­˜æ¨¡å‹
        model_path = MODELS_DIR / f"xgboost_model_v4_{h}h.pkl"
        with open(model_path, 'wb') as f:
            pickle.dump(model, f)

        # ä¿å­˜ç‰¹å¾é‡è¦æ€§
        imp_df = pd.DataFrame(report['features'])
        imp_df.to_csv(MODELS_DIR / f"feature_importance_v4_{h}h.csv", index=False)

    # ä¿å­˜å®Œæ•´æŠ¥å‘Šï¼ˆä¾› Dashboard ä½¿ç”¨ï¼‰
    report_path = DASHBOARD_DATA_DIR / "interpretability_report.json"
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(all_reports, f, ensure_ascii=False, indent=2)
    print(f"\nğŸ’¾ å¯è§£é‡Šæ€§æŠ¥å‘Šå·²ä¿å­˜: {report_path}")

    # æ±‡æ€»
    print("\n" + "=" * 70)
    print("  æ¨¡å‹æ€§èƒ½æ±‡æ€»")
    print("=" * 70)
    print(f"\n   é¢„æµ‹æ—¶é—´ |   MAE   |  RMSE   |   RÂ²")
    print("   " + "-" * 40)
    for h in horizons:
        if h in all_reports:
            m = all_reports[h]['metrics']
            print(f"      {h:2}h   |  {m['MAE']:5.2f}  |  {m['RMSE']:5.2f}  | {m['R2']:6.3f}")

    print("\n" + "=" * 70)
    print("  å…³é”®å‘ç°ï¼ˆæ‰¬å·ç‰¹æœ‰è§„å¾‹ï¼‰")
    print("=" * 70)
    print("""
    1. æ¹¿åº¦ä¸AQIè´Ÿç›¸å…³ï¼ˆä¸åŒ—æ–¹åŸå¸‚ç›¸åï¼‰
       - é«˜æ¹¿åº¦(>80%)æ—¶å¹³å‡AQI=49ï¼Œå¹²ç‡¥(<40%)æ—¶AQI=68
       - åŸå› ï¼šæ‰¬å·ä½äºé•¿ä¸‰è§’ï¼Œé«˜æ¹¿åº¦é€šå¸¸ä¼´éšé™é›¨/æµ·æ´‹æ°”å›¢

    2. æ—¥æ¸©å·®ä¸AQIæ­£ç›¸å…³
       - æ—¥æ¸©å·®å¤§=æ™´æœ—å¤©æ°”=é«˜å‹æ§åˆ¶=å¤§æ°”ç¨³å®š=ä¸åˆ©æ‰©æ•£
       - è¿™ä¸"é€†æ¸©å±‚"å‡è®¾ä¸åŒï¼Œåæ˜ äº†é•¿ä¸‰è§’çš„å®é™…æƒ…å†µ

    3. ä¾›æš–å­£æ•ˆåº”æ˜¾è‘—
       - 11-3æœˆAQIæ˜æ˜¾åé«˜ï¼Œåæ˜ åŒ—æ–¹ä¾›æš–å¸¦æ¥çš„åŒºåŸŸä¼ è¾“

    è¿™äº›å‘ç°å±•ç¤ºäº†å¯è§£é‡Šæ€§åˆ†æçš„ä»·å€¼ï¼š
    å‘ç°æ•°æ®ä¸­çš„çœŸå®è§„å¾‹ï¼Œè€Œéç®€å•å¥—ç”¨æ•™ç§‘ä¹¦çŸ¥è¯†ã€‚
    """)


if __name__ == "__main__":
    main()
