# -*- coding: utf-8 -*-
"""
æ¨¡å‹è®­ç»ƒæ¨¡å—
åŒ…æ‹¬åŸºçº¿æ¨¡å‹å’Œä¸»æ¨¡å‹è®­ç»ƒ
"""

import sys
import json
import pickle
import numpy as np
import pandas as pd
from pathlib import Path
from datetime import datetime
from sklearn.model_selection import TimeSeriesSplit
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent.parent))
from src.config import FEATURES_DATA_DIR, MODELS_DIR, DOCS_DIR

# å°è¯•å¯¼å…¥ XGBoost å’Œ LightGBM
try:
    import xgboost as xgb
    HAS_XGBOOST = True
except ImportError:
    HAS_XGBOOST = False
    print("âš ï¸ XGBoost æœªå®‰è£…ï¼Œå°†ä½¿ç”¨ GradientBoosting ä»£æ›¿")

try:
    import lightgbm as lgb
    HAS_LIGHTGBM = True
except ImportError:
    HAS_LIGHTGBM = False


def calculate_metrics(y_true, y_pred):
    """
    è®¡ç®—è¯„ä¼°æŒ‡æ ‡

    Args:
        y_true: çœŸå®å€¼
        y_pred: é¢„æµ‹å€¼

    Returns:
        dict: è¯„ä¼°æŒ‡æ ‡
    """
    # è¿‡æ»¤æ— æ•ˆå€¼
    mask = ~(np.isnan(y_true) | np.isnan(y_pred))
    y_true = y_true[mask]
    y_pred = y_pred[mask]

    mae = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    r2 = r2_score(y_true, y_pred)

    # MAPE (é¿å…é™¤ä»¥é›¶)
    mask = y_true != 0
    if mask.sum() > 0:
        mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100
    else:
        mape = np.nan

    return {
        'MAE': round(mae, 2),
        'RMSE': round(rmse, 2),
        'R2': round(r2, 4),
        'MAPE': round(mape, 2) if not np.isnan(mape) else None
    }


class BaselineModels:
    """åŸºçº¿æ¨¡å‹"""

    @staticmethod
    def naive_yesterday(y, hours_lag=24):
        """
        å¤©çœŸé¢„æµ‹ï¼šä½¿ç”¨æ˜¨å¤©åŒä¸€æ—¶åˆ»çš„å€¼

        Args:
            y: ç›®æ ‡åºåˆ—
            hours_lag: æ»åå°æ—¶æ•°

        Returns:
            é¢„æµ‹å€¼
        """
        return y.shift(hours_lag)

    @staticmethod
    def moving_average(y, window=24):
        """
        ç§»åŠ¨å¹³å‡é¢„æµ‹

        Args:
            y: ç›®æ ‡åºåˆ—
            window: çª—å£å¤§å°

        Returns:
            é¢„æµ‹å€¼
        """
        return y.rolling(window=window, min_periods=1).mean().shift(1)

    @staticmethod
    def exponential_smoothing(y, alpha=0.3):
        """
        æŒ‡æ•°å¹³æ»‘é¢„æµ‹

        Args:
            y: ç›®æ ‡åºåˆ—
            alpha: å¹³æ»‘ç³»æ•°

        Returns:
            é¢„æµ‹å€¼
        """
        result = y.copy()
        for i in range(1, len(y)):
            result.iloc[i] = alpha * y.iloc[i-1] + (1 - alpha) * result.iloc[i-1]
        return result.shift(1)


def train_baseline_models(df, target_col='AQI'):
    """
    è®­ç»ƒå’Œè¯„ä¼°åŸºçº¿æ¨¡å‹

    Args:
        df: DataFrame
        target_col: ç›®æ ‡å˜é‡åˆ—å

    Returns:
        dict: åŸºçº¿æ¨¡å‹ç»“æœ
    """
    print("\nğŸ“Š è®­ç»ƒåŸºçº¿æ¨¡å‹...")

    y = df[target_col].copy()

    results = {}

    # 1. å¤©çœŸé¢„æµ‹ï¼ˆæ˜¨å¤©åŒä¸€æ—¶åˆ»ï¼‰
    pred_naive = BaselineModels.naive_yesterday(y, 24)
    valid_idx = ~(pred_naive.isna() | y.isna())
    results['Naive_24h'] = calculate_metrics(y[valid_idx].values, pred_naive[valid_idx].values)
    print(f"   Naive (24h): MAE={results['Naive_24h']['MAE']}, RMSE={results['Naive_24h']['RMSE']}")

    # 2. ç§»åŠ¨å¹³å‡
    pred_ma = BaselineModels.moving_average(y, 24)
    valid_idx = ~(pred_ma.isna() | y.isna())
    results['MovingAvg_24h'] = calculate_metrics(y[valid_idx].values, pred_ma[valid_idx].values)
    print(f"   Moving Avg (24h): MAE={results['MovingAvg_24h']['MAE']}, RMSE={results['MovingAvg_24h']['RMSE']}")

    # 3. æŒ‡æ•°å¹³æ»‘
    pred_es = BaselineModels.exponential_smoothing(y, 0.3)
    valid_idx = ~(pred_es.isna() | y.isna())
    results['ExpSmoothing'] = calculate_metrics(y[valid_idx].values, pred_es[valid_idx].values)
    print(f"   Exp Smoothing: MAE={results['ExpSmoothing']['MAE']}, RMSE={results['ExpSmoothing']['RMSE']}")

    # 4. å•ç‰¹å¾çº¿æ€§å›å½’ï¼ˆä½¿ç”¨ 1 å°æ—¶æ»åçš„ AQIï¼‰
    if 'AQI_lag_1h' in df.columns:
        X_simple = df[['AQI_lag_1h']].dropna()
        y_simple = df.loc[X_simple.index, target_col]

        # ç®€å•è®­ç»ƒæµ‹è¯•åˆ†å‰²ï¼ˆ80/20ï¼‰
        split_idx = int(len(X_simple) * 0.8)
        X_train, X_test = X_simple.iloc[:split_idx], X_simple.iloc[split_idx:]
        y_train, y_test = y_simple.iloc[:split_idx], y_simple.iloc[split_idx:]

        lr = LinearRegression()
        lr.fit(X_train, y_train)
        pred_lr = lr.predict(X_test)

        results['Linear_Lag1h'] = calculate_metrics(y_test.values, pred_lr)
        print(f"   Linear (lag 1h): MAE={results['Linear_Lag1h']['MAE']}, RMSE={results['Linear_Lag1h']['RMSE']}")

    return results


def train_main_model(df, target_col='AQI', feature_cols=None, n_splits=5):
    """
    è®­ç»ƒä¸»æ¨¡å‹

    Args:
        df: DataFrame
        target_col: ç›®æ ‡å˜é‡åˆ—å
        feature_cols: ç‰¹å¾åˆ—ååˆ—è¡¨
        n_splits: æ—¶é—´åºåˆ—äº¤å‰éªŒè¯æŠ˜æ•°

    Returns:
        tuple: (model, metrics, feature_importance)
    """
    print("\nğŸš€ è®­ç»ƒä¸»æ¨¡å‹...")

    # å‡†å¤‡ç‰¹å¾
    if feature_cols is None:
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        feature_cols = [c for c in numeric_cols if c != target_col and c != 'datetime']

    # åˆ é™¤å«æœ‰æ— ç©·å€¼å’Œç¼ºå¤±å€¼çš„è¡Œ
    df_clean = df[feature_cols + [target_col]].replace([np.inf, -np.inf], np.nan).dropna()

    X = df_clean[feature_cols]
    y = df_clean[target_col]

    print(f"   ç‰¹å¾æ•°: {len(feature_cols)}")
    print(f"   æ ·æœ¬æ•°: {len(X)}")

    # æ—¶é—´åºåˆ—äº¤å‰éªŒè¯
    tscv = TimeSeriesSplit(n_splits=n_splits)

    # é€‰æ‹©æ¨¡å‹
    if HAS_XGBOOST:
        print("   ä½¿ç”¨ XGBoost æ¨¡å‹")
        model = xgb.XGBRegressor(
            n_estimators=200,
            max_depth=6,
            learning_rate=0.1,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=42,
            n_jobs=-1
        )
    else:
        print("   ä½¿ç”¨ GradientBoosting æ¨¡å‹")
        model = GradientBoostingRegressor(
            n_estimators=200,
            max_depth=6,
            learning_rate=0.1,
            subsample=0.8,
            random_state=42
        )

    # äº¤å‰éªŒè¯
    cv_results = []
    all_predictions = []
    all_actuals = []

    for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):
        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

        model.fit(X_train, y_train)
        y_pred = model.predict(X_val)

        metrics = calculate_metrics(y_val.values, y_pred)
        cv_results.append(metrics)
        all_predictions.extend(y_pred)
        all_actuals.extend(y_val.values)

        print(f"   Fold {fold+1}: MAE={metrics['MAE']}, RMSE={metrics['RMSE']}, RÂ²={metrics['R2']}")

    # è®¡ç®—å¹³å‡æŒ‡æ ‡
    avg_metrics = {
        'MAE': round(np.mean([r['MAE'] for r in cv_results]), 2),
        'RMSE': round(np.mean([r['RMSE'] for r in cv_results]), 2),
        'R2': round(np.mean([r['R2'] for r in cv_results]), 4),
        'MAPE': round(np.mean([r['MAPE'] for r in cv_results if r['MAPE'] is not None]), 2)
    }

    print(f"\n   å¹³å‡æŒ‡æ ‡: MAE={avg_metrics['MAE']}, RMSE={avg_metrics['RMSE']}, RÂ²={avg_metrics['R2']}")

    # åœ¨å…¨éƒ¨æ•°æ®ä¸Šé‡æ–°è®­ç»ƒæœ€ç»ˆæ¨¡å‹
    model.fit(X, y)

    # è·å–ç‰¹å¾é‡è¦æ€§
    if hasattr(model, 'feature_importances_'):
        importance_df = pd.DataFrame({
            'feature': feature_cols,
            'importance': model.feature_importances_
        }).sort_values('importance', ascending=False)
    else:
        importance_df = None

    return model, avg_metrics, importance_df, cv_results


def train_ablation_models(df, target_col='AQI', feature_cols=None):
    """
    æ¶ˆèå®éªŒï¼šè¯„ä¼°ä¸åŒç‰¹å¾ç»„çš„è´¡çŒ®

    Args:
        df: DataFrame
        target_col: ç›®æ ‡å˜é‡åˆ—å
        feature_cols: å…¨éƒ¨ç‰¹å¾åˆ—ååˆ—è¡¨

    Returns:
        dict: æ¶ˆèå®éªŒç»“æœ
    """
    print("\nğŸ”¬ è¿›è¡Œæ¶ˆèå®éªŒ...")

    if feature_cols is None:
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        feature_cols = [c for c in numeric_cols if c != target_col]

    # ç‰¹å¾åˆ†ç»„
    feature_groups = {
        'æ—¶é—´ç‰¹å¾': [c for c in feature_cols if any(x in c for x in ['hour', 'day', 'month', 'week', 'season', 'sin', 'cos', 'period', 'weekend', 'holiday', 'workday'])],
        'æ»åç‰¹å¾': [c for c in feature_cols if 'lag' in c],
        'æ»šåŠ¨ç‰¹å¾': [c for c in feature_cols if 'rolling' in c],
        'æ°”è±¡ç‰¹å¾': [c for c in feature_cols if any(x in c for x in ['temp', 'humidity', 'wind', 'pressure', 'precipitation', 'cloud', 'rain'])],
        'å˜åŒ–ç‡ç‰¹å¾': [c for c in feature_cols if 'change' in c or 'pct' in c],
        'ä»£ç†å˜é‡': [c for c in feature_cols if 'proxy' in c or 'inversion' in c or 'burning' in c or 'festival' in c or 'heating' in c],
    }

    # åˆ é™¤ç©ºç»„
    feature_groups = {k: v for k, v in feature_groups.items() if len(v) > 0}

    results = {}

    # å‡†å¤‡æ•°æ®
    df_clean = df[feature_cols + [target_col]].replace([np.inf, -np.inf], np.nan).dropna()
    X = df_clean[feature_cols]
    y = df_clean[target_col]

    # 1. å®Œæ•´æ¨¡å‹
    print("   è®­ç»ƒå®Œæ•´æ¨¡å‹...")
    full_model = GradientBoostingRegressor(n_estimators=100, max_depth=5, random_state=42)
    tscv = TimeSeriesSplit(n_splits=3)

    cv_scores = []
    for train_idx, val_idx in tscv.split(X):
        full_model.fit(X.iloc[train_idx], y.iloc[train_idx])
        pred = full_model.predict(X.iloc[val_idx])
        cv_scores.append(calculate_metrics(y.iloc[val_idx].values, pred))

    results['å®Œæ•´æ¨¡å‹'] = {
        'features_used': len(feature_cols),
        'MAE': round(np.mean([s['MAE'] for s in cv_scores]), 2),
        'RMSE': round(np.mean([s['RMSE'] for s in cv_scores]), 2),
        'R2': round(np.mean([s['R2'] for s in cv_scores]), 4)
    }

    # 2. ç§»é™¤æ¯ç»„ç‰¹å¾åçš„æ¨¡å‹
    for group_name, group_features in feature_groups.items():
        remaining_features = [c for c in feature_cols if c not in group_features]

        if len(remaining_features) == 0:
            continue

        print(f"   è®­ç»ƒå»é™¤ [{group_name}] åçš„æ¨¡å‹...")

        X_ablation = df_clean[remaining_features]

        cv_scores = []
        for train_idx, val_idx in tscv.split(X_ablation):
            model = GradientBoostingRegressor(n_estimators=100, max_depth=5, random_state=42)
            model.fit(X_ablation.iloc[train_idx], y.iloc[train_idx])
            pred = model.predict(X_ablation.iloc[val_idx])
            cv_scores.append(calculate_metrics(y.iloc[val_idx].values, pred))

        results[f'å»é™¤{group_name}'] = {
            'features_removed': len(group_features),
            'features_used': len(remaining_features),
            'MAE': round(np.mean([s['MAE'] for s in cv_scores]), 2),
            'RMSE': round(np.mean([s['RMSE'] for s in cv_scores]), 2),
            'R2': round(np.mean([s['R2'] for s in cv_scores]), 4)
        }

        # è®¡ç®—è¯¥ç»„ç‰¹å¾çš„è´¡çŒ®
        mae_increase = results[f'å»é™¤{group_name}']['MAE'] - results['å®Œæ•´æ¨¡å‹']['MAE']
        results[f'å»é™¤{group_name}']['MAE_increase'] = round(mae_increase, 2)

    return results


def save_model(model, model_name='xgboost_model'):
    """ä¿å­˜æ¨¡å‹"""
    # ä¿å­˜ä¸º pickle
    pkl_path = MODELS_DIR / f"{model_name}.pkl"
    with open(pkl_path, 'wb') as f:
        pickle.dump(model, f)

    # å¦‚æœæ˜¯ XGBoostï¼Œä¹Ÿä¿å­˜ä¸º JSON
    if HAS_XGBOOST and isinstance(model, xgb.XGBRegressor):
        json_path = MODELS_DIR / f"{model_name}.json"
        model.save_model(str(json_path))

    print(f"âœ… æ¨¡å‹å·²ä¿å­˜: {pkl_path}")


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("  æ¨¡å‹è®­ç»ƒ")
    print("=" * 60)

    # è¯»å–ç‰¹å¾æ•°æ®
    input_file = FEATURES_DATA_DIR / "yangzhou_features_selected.csv"

    if not input_file.exists():
        # å°è¯•ä½¿ç”¨å®Œæ•´ç‰¹å¾æ–‡ä»¶
        input_file = FEATURES_DATA_DIR / "yangzhou_features.csv"

    if not input_file.exists():
        print(f"âŒ ç‰¹å¾æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
        return

    df = pd.read_csv(input_file)
    df['datetime'] = pd.to_datetime(df['datetime'])
    print(f"ğŸ“– è¯»å–æ•°æ®: {len(df)} è¡Œ, {len(df.columns)} åˆ—")

    # è·å–ç‰¹å¾åˆ—
    feature_cols = [c for c in df.columns if c not in ['datetime', 'AQI', 'date']]

    # 1. è®­ç»ƒåŸºçº¿æ¨¡å‹
    baseline_results = train_baseline_models(df)

    # 2. è®­ç»ƒä¸»æ¨¡å‹
    model, main_metrics, importance_df, cv_results = train_main_model(df, feature_cols=feature_cols)

    # 3. æ¶ˆèå®éªŒ
    ablation_results = train_ablation_models(df, feature_cols=feature_cols)

    # ä¿å­˜æ¨¡å‹
    save_model(model)

    # ä¿å­˜è¯„ä¼°ç»“æœ
    evaluation = {
        'timestamp': datetime.now().isoformat(),
        'data_rows': len(df),
        'feature_count': len(feature_cols),
        'baseline_results': baseline_results,
        'main_model': {
            'type': 'XGBoost' if HAS_XGBOOST else 'GradientBoosting',
            'cv_folds': 5,
            'metrics': main_metrics,
            'cv_results': cv_results
        },
        'ablation_study': ablation_results
    }

    eval_file = DOCS_DIR / "model_evaluation.json"
    with open(eval_file, 'w', encoding='utf-8') as f:
        json.dump(evaluation, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ“„ è¯„ä¼°ç»“æœå·²ä¿å­˜: {eval_file}")

    # ä¿å­˜åŸºçº¿å¯¹æ¯”
    baseline_file = DOCS_DIR / "baseline_comparison.json"
    comparison = {
        'baseline_models': baseline_results,
        'main_model': main_metrics,
        'improvement': {
            'vs_naive': round(baseline_results.get('Naive_24h', {}).get('MAE', 0) - main_metrics['MAE'], 2),
            'vs_moving_avg': round(baseline_results.get('MovingAvg_24h', {}).get('MAE', 0) - main_metrics['MAE'], 2),
        }
    }
    with open(baseline_file, 'w', encoding='utf-8') as f:
        json.dump(comparison, f, ensure_ascii=False, indent=2)

    print(f"ğŸ“„ åŸºçº¿å¯¹æ¯”å·²ä¿å­˜: {baseline_file}")

    # ä¿å­˜æ¶ˆèå®éªŒç»“æœ
    ablation_file = DOCS_DIR / "ablation_study.json"
    with open(ablation_file, 'w', encoding='utf-8') as f:
        json.dump(ablation_results, f, ensure_ascii=False, indent=2)

    print(f"ğŸ“„ æ¶ˆèå®éªŒç»“æœå·²ä¿å­˜: {ablation_file}")

    # ä¿å­˜ç‰¹å¾é‡è¦æ€§
    if importance_df is not None:
        importance_file = DOCS_DIR / "feature_importance.json"
        importance_df.to_json(importance_file, orient='records', force_ascii=False, indent=2)
        print(f"ğŸ“„ ç‰¹å¾é‡è¦æ€§å·²ä¿å­˜: {importance_file}")

    print("\nâœ… æ¨¡å‹è®­ç»ƒå®Œæˆ!")

    return model, evaluation


if __name__ == "__main__":
    main()
