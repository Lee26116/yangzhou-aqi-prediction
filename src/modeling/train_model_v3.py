# -*- coding: utf-8 -*-
"""
æ¨¡å‹è®­ç»ƒ v3 - æ–¹æ³•è®ºæ­£ç¡®ç‰ˆæœ¬

ä¸ºæ¯ä¸ªé¢„æµ‹æ—¶é—´è·¨åº¦ï¼ˆ1h, 6h, 12h, 24hï¼‰è®­ç»ƒç‹¬ç«‹çš„æ¨¡å‹
æ¯ä¸ªæ¨¡å‹åªä½¿ç”¨è¯¥æ—¶é—´è·¨åº¦ä¸‹åˆæ³•å¯çŸ¥çš„ç‰¹å¾
"""

import sys
import pickle
import numpy as np
import pandas as pd
from pathlib import Path
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from xgboost import XGBRegressor

sys.path.insert(0, str(Path(__file__).parent.parent.parent))
from src.config import FEATURES_DATA_DIR, MODELS_DIR


def load_data(horizon_hours):
    """åŠ è½½ç‰¹å®šé¢„æµ‹æ—¶é—´è·¨åº¦çš„ç‰¹å¾æ•°æ®"""
    input_file = FEATURES_DATA_DIR / f"yangzhou_features_v3_{horizon_hours}h.csv"

    if not input_file.exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
        print(f"   è¯·å…ˆè¿è¡Œ build_features_v3.py")
        return None, None, None

    df = pd.read_csv(input_file)
    df['datetime'] = pd.to_datetime(df['datetime'])
    df = df.sort_values('datetime').reset_index(drop=True)

    # åˆ†ç¦»ç‰¹å¾å’Œç›®æ ‡
    feature_cols = [c for c in df.columns if c not in ['datetime', 'AQI_target']]
    X = df[feature_cols].fillna(df[feature_cols].median())
    y = df['AQI_target']

    return X, y, feature_cols


def train_baseline(X, y, horizon_hours):
    """è®­ç»ƒåŸºçº¿æ¨¡å‹"""
    print(f"\nğŸ“Š åŸºçº¿æ¨¡å‹è¯„ä¼° ({horizon_hours}h é¢„æµ‹)...")

    results = {}

    # Naive: ç”¨ lag_0hï¼ˆå³ horizon_hours å‰çš„ AQIï¼‰ä½œä¸ºé¢„æµ‹
    if 'AQI_lag_0h' in X.columns:
        naive_pred = X['AQI_lag_0h'].values
        valid_idx = ~np.isnan(naive_pred)
        if valid_idx.sum() > 0:
            naive_mae = mean_absolute_error(y[valid_idx], naive_pred[valid_idx])
            naive_r2 = r2_score(y[valid_idx], naive_pred[valid_idx])
            results['Naive (lag_0h)'] = {'MAE': naive_mae, 'R2': naive_r2}
            print(f"   Naive (ç”¨ {horizon_hours}h å‰çš„ AQI): MAE={naive_mae:.2f}, RÂ²={naive_r2:.4f}")

    # å†å²åŒæœŸ
    if 'AQI_yesterday_same_hour' in X.columns:
        hist_pred = X['AQI_yesterday_same_hour'].values
        valid_idx = ~np.isnan(hist_pred)
        if valid_idx.sum() > 0:
            hist_mae = mean_absolute_error(y[valid_idx], hist_pred[valid_idx])
            hist_r2 = r2_score(y[valid_idx], hist_pred[valid_idx])
            results['Historical (æ˜¨å¤©åŒæœŸ)'] = {'MAE': hist_mae, 'R2': hist_r2}
            print(f"   Historical (æ˜¨å¤©åŒæœŸ): MAE={hist_mae:.2f}, RÂ²={hist_r2:.4f}")

    return results


def train_xgboost(X, y, feature_cols, horizon_hours):
    """è®­ç»ƒ XGBoost æ¨¡å‹"""
    print(f"\nğŸ¤– è®­ç»ƒ XGBoost æ¨¡å‹ ({horizon_hours}h é¢„æµ‹)...")

    # æ—¶åºäº¤å‰éªŒè¯
    tscv = TimeSeriesSplit(n_splits=5)

    cv_results = {'MAE': [], 'RMSE': [], 'R2': [], 'MAPE': []}

    # XGBoost å‚æ•°
    params = {
        'n_estimators': 200,
        'max_depth': 6,
        'learning_rate': 0.1,
        'subsample': 0.8,
        'colsample_bytree': 0.8,
        'random_state': 42,
        'n_jobs': -1
    }

    print(f"   5-fold æ—¶åºäº¤å‰éªŒè¯...")

    for fold, (train_idx, test_idx) in enumerate(tscv.split(X)):
        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]

        model = XGBRegressor(**params)
        model.fit(X_train, y_train, verbose=False)

        y_pred = model.predict(X_test)

        mae = mean_absolute_error(y_test, y_pred)
        rmse = np.sqrt(mean_squared_error(y_test, y_pred))
        r2 = r2_score(y_test, y_pred)

        # å®‰å…¨è®¡ç®— MAPE
        mask = y_test > 10  # é¿å…é™¤ä»¥æ¥è¿‘é›¶çš„å€¼
        if mask.sum() > 0:
            mape = np.mean(np.abs((y_test[mask] - y_pred[mask]) / y_test[mask])) * 100
        else:
            mape = np.nan

        cv_results['MAE'].append(mae)
        cv_results['RMSE'].append(rmse)
        cv_results['R2'].append(r2)
        cv_results['MAPE'].append(mape)

        print(f"      Fold {fold+1}: MAE={mae:.2f}, RMSE={rmse:.2f}, RÂ²={r2:.4f}")

    # æ‰“å°å¹³å‡ç»“æœ
    print(f"\n   ğŸ“ˆ äº¤å‰éªŒè¯å¹³å‡ç»“æœ:")
    print(f"      MAE:  {np.mean(cv_results['MAE']):.2f} Â± {np.std(cv_results['MAE']):.2f}")
    print(f"      RMSE: {np.mean(cv_results['RMSE']):.2f} Â± {np.std(cv_results['RMSE']):.2f}")
    print(f"      RÂ²:   {np.mean(cv_results['R2']):.4f} Â± {np.std(cv_results['R2']):.4f}")
    print(f"      MAPE: {np.nanmean(cv_results['MAPE']):.2f}%")

    # è®­ç»ƒæœ€ç»ˆæ¨¡å‹
    print(f"\n   è®­ç»ƒæœ€ç»ˆæ¨¡å‹...")
    final_model = XGBRegressor(**params)
    final_model.fit(X, y, verbose=False)

    return final_model, cv_results


def analyze_feature_importance(model, feature_cols, horizon_hours):
    """åˆ†æç‰¹å¾é‡è¦æ€§"""
    print(f"\nğŸ“Š ç‰¹å¾é‡è¦æ€§åˆ†æ ({horizon_hours}h)...")

    importance = model.feature_importances_
    imp_df = pd.DataFrame({
        'feature': feature_cols,
        'importance': importance
    }).sort_values('importance', ascending=False)

    print("   Top 10 é‡è¦ç‰¹å¾:")
    for i, row in imp_df.head(10).iterrows():
        pct = row['importance'] * 100
        bar = 'â–ˆ' * int(pct * 2)
        print(f"      {row['feature']:35} {pct:5.2f}% {bar}")

    return imp_df


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("  æ¨¡å‹è®­ç»ƒ v3 - æ–¹æ³•è®ºæ­£ç¡®ç‰ˆæœ¬")
    print("  ä¸ºæ¯ä¸ªé¢„æµ‹æ—¶é—´è·¨åº¦è®­ç»ƒç‹¬ç«‹æ¨¡å‹")
    print("=" * 60)

    # é¢„æµ‹æ—¶é—´è·¨åº¦
    horizons = [1, 6, 12, 24]

    all_results = {}

    for h in horizons:
        print(f"\n{'='*60}")
        print(f"  é¢„æµ‹æ—¶é—´è·¨åº¦: {h} å°æ—¶")
        print(f"{'='*60}")

        # åŠ è½½æ•°æ®
        X, y, feature_cols = load_data(h)
        if X is None:
            continue

        print(f"\nğŸ“‚ æ•°æ®åŠ è½½å®Œæˆ:")
        print(f"   æ ·æœ¬æ•°: {len(X)}")
        print(f"   ç‰¹å¾æ•°: {len(feature_cols)}")

        # åŸºçº¿æ¨¡å‹
        baseline_results = train_baseline(X, y, h)

        # XGBoost
        model, cv_results = train_xgboost(X, y, feature_cols, h)

        # ç‰¹å¾é‡è¦æ€§
        imp_df = analyze_feature_importance(model, feature_cols, h)

        # ä¿å­˜æ¨¡å‹
        model_path = MODELS_DIR / f"xgboost_model_v3_{h}h.pkl"
        with open(model_path, 'wb') as f:
            pickle.dump(model, f)
        print(f"\nğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {model_path}")

        # ä¿å­˜ç‰¹å¾é‡è¦æ€§
        imp_path = MODELS_DIR / f"feature_importance_v3_{h}h.csv"
        imp_df.to_csv(imp_path, index=False)

        # è®°å½•ç»“æœ
        all_results[h] = {
            'baseline': baseline_results,
            'xgboost': {
                'MAE': np.mean(cv_results['MAE']),
                'RMSE': np.mean(cv_results['RMSE']),
                'R2': np.mean(cv_results['R2']),
                'MAPE': np.nanmean(cv_results['MAPE'])
            },
            'improvement': None
        }

        # è®¡ç®—ç›¸å¯¹åŸºçº¿çš„æå‡
        if 'Naive (lag_0h)' in baseline_results:
            baseline_mae = baseline_results['Naive (lag_0h)']['MAE']
            xgb_mae = np.mean(cv_results['MAE'])
            improvement = (baseline_mae - xgb_mae) / baseline_mae * 100
            all_results[h]['improvement'] = improvement

    # æ±‡æ€»æŠ¥å‘Š
    print("\n" + "=" * 60)
    print("  æ¨¡å‹è®­ç»ƒæ±‡æ€»")
    print("=" * 60)

    print("\né¢„æµ‹æ—¶é—´ | Naive MAE | XGBoost MAE | RÂ² | æå‡")
    print("-" * 60)

    for h in horizons:
        if h not in all_results:
            continue
        r = all_results[h]
        naive_mae = r['baseline'].get('Naive (lag_0h)', {}).get('MAE', np.nan)
        xgb_mae = r['xgboost']['MAE']
        r2 = r['xgboost']['R2']
        imp = r['improvement'] if r['improvement'] else 0

        print(f"   {h:2}h    |   {naive_mae:5.2f}   |    {xgb_mae:5.2f}    | {r2:.3f} | {imp:+5.1f}%")

    print("\n" + "=" * 60)
    print("  å…³é”®å‘ç°")
    print("=" * 60)
    print("""
    1. éšç€é¢„æµ‹æ—¶é—´å¢åŠ ï¼ŒMAE ä¼šä¸Šå‡ï¼ŒRÂ² ä¼šä¸‹é™ â€”â€” è¿™æ˜¯åˆç†çš„
    2. Naive baseline (ç”¨å†å²å€¼é¢„æµ‹) æ˜¯é‡è¦çš„å¯¹æ¯”åŸºå‡†
    3. å¦‚æœ XGBoost ç›¸æ¯” Naive æå‡ä¸å¤§ï¼Œè¯´æ˜ AQI ä¸»è¦é æƒ¯æ€§
    4. è¿™ä¸ªç»“æœæ˜¯è¯šå®çš„ï¼šæ²¡æœ‰ç”¨ä¸å¯çŸ¥çš„ä¿¡æ¯æ¥"ä½œå¼Š"
    """)


if __name__ == "__main__":
    main()
