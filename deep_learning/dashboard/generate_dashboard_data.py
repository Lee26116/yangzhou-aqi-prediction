# -*- coding: utf-8 -*-
"""
æ·±åº¦å­¦ä¹  Dashboard æ•°æ®ç”Ÿæˆ
ç”Ÿæˆ dl_overview.json, model_comparison.json, training_curves.json, attention_samples.json
"""

import sys
import json
import numpy as np
import pandas as pd
from datetime import timedelta
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent.parent))
from src.config import DASHBOARD_DATA_DIR, get_aqi_level, get_china_now
from deep_learning.dl_config import (
    DL_MODELS_DIR, DL_SEQUENCES_DIR, DL_FEATURES_DIR, PREDICTION_HORIZONS
)


def generate_training_curves():
    """ç”Ÿæˆè®­ç»ƒ/éªŒè¯ loss æ›²çº¿æ•°æ®"""
    print("ğŸ“ˆ ç”Ÿæˆè®­ç»ƒæ›²çº¿æ•°æ®...")

    history_file = DL_MODELS_DIR / "training_history.json"
    if not history_file.exists():
        print("   âš ï¸ è®­ç»ƒå†å²ä¸å­˜åœ¨ï¼Œè·³è¿‡")
        return None

    with open(history_file, 'r', encoding='utf-8') as f:
        history = json.load(f)

    curves = {
        'epochs': list(range(1, len(history['train_loss']) + 1)),
        'train_loss': history['train_loss'],
        'val_loss': history['val_loss'],
        'learning_rate': history.get('lr', []),
        'best_epoch': None,
    }

    # æ ‡è®°æœ€ä½³ epoch
    if history['val_loss']:
        best_idx = int(np.argmin(history['val_loss']))
        curves['best_epoch'] = best_idx + 1
        curves['best_val_loss'] = history['val_loss'][best_idx]

    output_file = DASHBOARD_DATA_DIR / "training_curves.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(curves, f, ensure_ascii=False, indent=2, default=str)

    print(f"   âœ… è®­ç»ƒæ›²çº¿: {output_file}")
    return curves


def generate_attention_data():
    """ç”Ÿæˆ Attention çƒ­åŠ›å›¾æ•°æ®"""
    print("ğŸ¯ ç”Ÿæˆ Attention æ•°æ®...")

    attn_file = DL_MODELS_DIR / "attention_samples.json"
    if not attn_file.exists():
        print("   âš ï¸ Attention æ ·æœ¬ä¸å­˜åœ¨ï¼Œè·³è¿‡")
        return None

    with open(attn_file, 'r', encoding='utf-8') as f:
        samples = json.load(f)

    # åŠ è½½ç‰¹å¾å
    feature_names_file = DL_SEQUENCES_DIR / "feature_names.txt"
    feature_names = []
    if feature_names_file.exists():
        with open(feature_names_file, 'r', encoding='utf-8') as f:
            feature_names = [line.strip() for line in f.readlines()]

    # åŠ è½½å…ƒæ•°æ®
    meta_file = DL_SEQUENCES_DIR / "metadata.json"
    seq_len = 48
    if meta_file.exists():
        with open(meta_file, 'r', encoding='utf-8') as f:
            meta = json.load(f)
            seq_len = meta.get('sequence_length', 48)

    attention_data = {
        'sequence_length': seq_len,
        'time_labels': [f't-{seq_len - i}h' for i in range(seq_len)],
        'feature_names': feature_names,
        'samples': samples,
    }

    output_file = DASHBOARD_DATA_DIR / "attention_samples.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(attention_data, f, ensure_ascii=False, indent=2, default=str)

    print(f"   âœ… Attention æ•°æ®: {output_file}")
    return attention_data


def generate_model_comparison():
    """ç”Ÿæˆæ¨¡å‹å¯¹æ¯”æ•°æ®"""
    print("ğŸ“Š ç”Ÿæˆæ¨¡å‹å¯¹æ¯”æ•°æ®...")

    # æ£€æŸ¥æ˜¯å¦å·²åœ¨è¯„ä¼°é˜¶æ®µç”Ÿæˆ
    comparison_file = DASHBOARD_DATA_DIR / "model_comparison.json"
    if comparison_file.exists():
        print(f"   âœ… å¯¹æ¯”æ•°æ®å·²å­˜åœ¨: {comparison_file}")
        return

    # å°è¯•ä»è¯„ä¼°ç»“æœç”Ÿæˆ
    eval_file = DL_MODELS_DIR / "dl_evaluation.json"
    if eval_file.exists():
        with open(eval_file, 'r') as f:
            eval_data = json.load(f)

        comparison = eval_data.get('comparison', {})
        with open(comparison_file, 'w', encoding='utf-8') as f:
            json.dump(comparison, f, ensure_ascii=False, indent=2)

        print(f"   âœ… å¯¹æ¯”æ•°æ®: {comparison_file}")
    else:
        print("   âš ï¸ è¯„ä¼°ç»“æœä¸å­˜åœ¨ï¼Œè·³è¿‡")


def generate_dl_overview():
    """ç”Ÿæˆæ·±åº¦å­¦ä¹ æ¨¡å‹çš„é¢„æµ‹æ¦‚è§ˆæ•°æ® (dl_overview.json)"""
    print("ğŸ”® ç”Ÿæˆ DL é¢„æµ‹æ¦‚è§ˆ...")

    from deep_learning.inference.predict_dl import get_dl_predictor

    # åŠ è½½ DL ç‰¹å¾
    dl_features_file = DL_FEATURES_DIR / "dl_features.csv"
    if not dl_features_file.exists():
        print(f"   âš ï¸ DL ç‰¹å¾æ–‡ä»¶ä¸å­˜åœ¨: {dl_features_file}")
        return None

    df = pd.read_csv(dl_features_file)
    df['datetime'] = pd.to_datetime(df['datetime'])

    # è·å–æœ€è¿‘72å°æ—¶æ•°æ®
    latest_time = df['datetime'].max()
    start_time = latest_time - timedelta(hours=72)
    recent_data = df[df['datetime'] >= start_time].copy()

    if len(recent_data) < 48:
        print(f"   âš ï¸ æ•°æ®ä¸è¶³ (ä»… {len(recent_data)} è¡Œ, éœ€è¦ 48)")
        return None

    # åŠ è½½é¢„æµ‹å™¨
    try:
        predictor = get_dl_predictor()
        predictor.load()
    except Exception as e:
        print(f"   âš ï¸ DL æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return None

    # è·å–å®æ—¶ AQI
    realtime = None
    try:
        import requests
        from src.config import WAQI_TOKEN
        url = f"https://api.waqi.info/feed/yangzhou/?token={WAQI_TOKEN}"
        response = requests.get(url, timeout=10)
        data = response.json()
        if data.get('status') == 'ok':
            aqi_data = data['data']
            iaqi = aqi_data.get('iaqi', {})
            def get_iaqi_value(key):
                val = iaqi.get(key)
                return val.get('v') if isinstance(val, dict) else val
            realtime = {
                'aqi': aqi_data.get('aqi'),
                'pm25': get_iaqi_value('pm25'),
                'pm10': get_iaqi_value('pm10'),
                'temperature': get_iaqi_value('t'),
                'humidity': get_iaqi_value('h'),
                'time': aqi_data.get('time', {}).get('iso'),
                'dominant_pollutant': aqi_data.get('dominentpol'),
            }
    except Exception as e:
        print(f"   âš ï¸ è·å–å®æ—¶æ•°æ®å¤±è´¥: {e}")

    # MC Dropout é¢„æµ‹ï¼ˆå«ç½®ä¿¡åŒºé—´ï¼‰
    try:
        uncertainty_dict = predictor.predict_with_uncertainty(recent_data, n_samples=50)
        predictions_dict = {h: v['mean'] for h, v in uncertainty_dict.items()}
    except Exception as e:
        print(f"   âš ï¸ DL é¢„æµ‹å¤±è´¥: {e}")
        return None

    # æ„å»ºé¢„æµ‹åˆ—è¡¨ï¼ˆ4ä¸ªå…³é”®ç‚¹ä¹‹é—´æ’å€¼åˆ°24å°æ—¶ï¼‰
    china_now = get_china_now()
    current_time = china_now.replace(minute=0, second=0, microsecond=0)

    dl_predictions = []
    for h in range(1, 25):
        pred_time = current_time + timedelta(hours=h)

        if h in predictions_dict:
            pred_aqi = predictions_dict[h]
            pred_lower = uncertainty_dict[h]['lower']
            pred_upper = uncertainty_dict[h]['upper']
        else:
            lower_h = max(k for k in PREDICTION_HORIZONS if k <= h)
            upper_h = min(k for k in PREDICTION_HORIZONS if k >= h)
            if lower_h == upper_h:
                pred_aqi = predictions_dict[lower_h]
                pred_lower = uncertainty_dict[lower_h]['lower']
                pred_upper = uncertainty_dict[lower_h]['upper']
            else:
                ratio = (h - lower_h) / (upper_h - lower_h)
                pred_aqi = predictions_dict[lower_h] * (1 - ratio) + predictions_dict[upper_h] * ratio
                pred_lower = uncertainty_dict[lower_h]['lower'] * (1 - ratio) + uncertainty_dict[upper_h]['lower'] * ratio
                pred_upper = uncertainty_dict[lower_h]['upper'] * (1 - ratio) + uncertainty_dict[upper_h]['upper'] * ratio

        pred_aqi = max(10, pred_aqi)
        pred_lower = max(5, pred_lower)
        pred_upper = max(pred_aqi, pred_upper)
        level, color = get_aqi_level(pred_aqi)

        dl_predictions.append({
            'datetime': pred_time.isoformat(),
            'hour': h,
            'predicted_aqi': round(pred_aqi, 1),
            'lower_bound': round(pred_lower, 1),
            'upper_bound': round(pred_upper, 1),
            'level': level,
            'color': color,
        })

    dl_overview = {
        'update_time': china_now.isoformat(),
        'update_time_display': china_now.strftime('%Y-%m-%d %H:%M:%S') + ' (åŒ—äº¬æ—¶é—´)',
        'timezone': 'Asia/Shanghai (UTC+8)',
        'model': 'LSTM+Attention',
        'realtime': realtime,
        'predictions': dl_predictions,
        'key_predictions': {f'{h}h': predictions_dict.get(h) for h in PREDICTION_HORIZONS},
        'historical': {
            'last_24h': recent_data[['datetime', 'AQI']].tail(24).to_dict(orient='records')
            if 'AQI' in recent_data.columns else []
        }
    }

    overview_file = DASHBOARD_DATA_DIR / "dl_overview.json"
    with open(overview_file, 'w', encoding='utf-8') as f:
        json.dump(dl_overview, f, ensure_ascii=False, indent=2, default=str)

    print(f"   âœ… DL é¢„æµ‹æ¦‚è§ˆ: {overview_file}")
    return dl_overview


def generate_all():
    """ç”Ÿæˆæ‰€æœ‰ Dashboard æ•°æ®"""
    print("=" * 60)
    print("  æ·±åº¦å­¦ä¹  Dashboard æ•°æ®ç”Ÿæˆ")
    print("=" * 60)

    generate_dl_overview()
    generate_training_curves()
    generate_attention_data()
    generate_model_comparison()

    print(f"\nâœ… æ‰€æœ‰ Dashboard æ•°æ®å·²ç”Ÿæˆåˆ°: {DASHBOARD_DATA_DIR}")


def main():
    generate_all()


if __name__ == "__main__":
    main()
