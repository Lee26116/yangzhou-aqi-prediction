# -*- coding: utf-8 -*-
"""
ONNX æ¨¡å‹å¯¼å‡º
å¯¼å‡º AQIPredictorInferenceï¼ˆæ—  attention è¾“å‡ºï¼‰
ç”¨ onnxruntime éªŒè¯æ­£ç¡®æ€§
"""

import sys
import numpy as np
import torch
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent.parent))
from deep_learning.dl_config import (
    DL_MODELS_DIR, DL_EXPORTED_DIR, DL_SEQUENCES_DIR,
    ONNX_CONFIG, MODEL_CONFIG, SEQUENCE_LENGTH
)
from deep_learning.models.lstm_attention import (
    build_model, AQIPredictorInference
)


def export_to_onnx():
    """å¯¼å‡ºæ¨¡å‹ä¸º ONNX æ ¼å¼"""
    print("=" * 60)
    print("  ONNX æ¨¡å‹å¯¼å‡º")
    print("=" * 60)

    # åŠ è½½æœ€ä½³æ¨¡å‹
    print("\nğŸ“¦ åŠ è½½æœ€ä½³æ¨¡å‹...")
    checkpoint_path = DL_MODELS_DIR / "best_model.pt"
    if not checkpoint_path.exists():
        raise FileNotFoundError(f"æ¨¡å‹ä¸å­˜åœ¨: {checkpoint_path}")

    checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)
    input_size = checkpoint['input_size']

    model = build_model(input_size, checkpoint.get('model_config', MODEL_CONFIG))
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()

    # åˆ›å»ºæ¨ç†æ¨¡å‹ï¼ˆä¸è¿”å› attentionï¼‰
    print("\nğŸ”§ åˆ›å»ºæ¨ç†æ¨¡å‹...")
    inference_model = AQIPredictorInference(model)
    inference_model.eval()

    # åˆ›å»ºç¤ºä¾‹è¾“å…¥
    dummy_input = torch.randn(1, SEQUENCE_LENGTH, input_size)

    # å¯¼å‡º ONNX
    onnx_path = DL_EXPORTED_DIR / ONNX_CONFIG['model_filename']
    print(f"\nğŸ“¤ å¯¼å‡º ONNX åˆ°: {onnx_path}")

    torch.onnx.export(
        inference_model,
        dummy_input,
        str(onnx_path),
        opset_version=ONNX_CONFIG['opset_version'],
        input_names=['input'],
        output_names=['predictions'],
        dynamic_axes={
            'input': {0: 'batch_size'},
            'predictions': {0: 'batch_size'},
        },
        dynamo=False,
    )

    print(f"   æ–‡ä»¶å¤§å°: {onnx_path.stat().st_size / 1024 / 1024:.2f} MB")

    # éªŒè¯ ONNX æ¨¡å‹
    print("\nğŸ” éªŒè¯ ONNX æ¨¡å‹...")
    try:
        import onnx
        onnx_model = onnx.load(str(onnx_path))
        onnx.checker.check_model(onnx_model)
        print("   âœ… ONNX æ¨¡å‹ç»“æ„éªŒè¯é€šè¿‡")
    except ImportError:
        print("   âš ï¸ onnx æœªå®‰è£…ï¼Œè·³è¿‡ç»“æ„éªŒè¯")

    # ç”¨ onnxruntime éªŒè¯æ­£ç¡®æ€§
    print("\nğŸ” OnnxRuntime æ¨ç†éªŒè¯...")
    try:
        import onnxruntime as ort

        session = ort.InferenceSession(str(onnx_path))
        input_name = session.get_inputs()[0].name
        ort_inputs = {input_name: dummy_input.numpy()}
        ort_outputs = session.run(None, ort_inputs)

        # å¯¹æ¯” PyTorch æ¨ç†
        with torch.no_grad():
            torch_output = inference_model(dummy_input).numpy()

        max_diff = np.max(np.abs(torch_output - ort_outputs[0]))
        print(f"   PyTorch vs ONNX æœ€å¤§å·®å¼‚: {max_diff:.6f}")

        if max_diff < 1e-4:
            print("   âœ… æ¨ç†ç»“æœä¸€è‡´æ€§éªŒè¯é€šè¿‡")
        else:
            print(f"   âš ï¸ å·®å¼‚è¾ƒå¤§ï¼Œè¯·æ£€æŸ¥ (max_diff={max_diff:.6f})")

    except ImportError:
        print("   âš ï¸ onnxruntime æœªå®‰è£…ï¼Œè·³è¿‡æ¨ç†éªŒè¯")

    # ä¿å­˜å¯¼å‡ºå…ƒæ•°æ®
    import json
    meta = {
        'input_size': input_size,
        'sequence_length': SEQUENCE_LENGTH,
        'opset_version': ONNX_CONFIG['opset_version'],
        'model_file': ONNX_CONFIG['model_filename'],
        'source_epoch': checkpoint['epoch'],
        'source_val_loss': float(checkpoint['val_loss']),
    }
    meta_file = DL_EXPORTED_DIR / "export_metadata.json"
    with open(meta_file, 'w') as f:
        json.dump(meta, f, indent=2)

    print(f"\nâœ… ONNX å¯¼å‡ºå®Œæˆ!")
    return str(onnx_path)


def main():
    export_to_onnx()


if __name__ == "__main__":
    main()
