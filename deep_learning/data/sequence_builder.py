# -*- coding: utf-8 -*-
"""
åºåˆ—ç”Ÿæˆå™¨
æ»‘åŠ¨çª—å£ï¼š48å°æ—¶å†å² â†’ é¢„æµ‹ [1h, 6h, 12h, 24h]
StandardScaler å½’ä¸€åŒ–ï¼ˆä»… fit è®­ç»ƒé›†ï¼‰
æ—¶åºåˆ†å‰²ï¼š80/10/10ï¼ˆä¸æ‰“ä¹±é¡ºåºï¼‰
"""

import sys
import pickle
import numpy as np
import pandas as pd
from pathlib import Path
from sklearn.preprocessing import StandardScaler

sys.path.insert(0, str(Path(__file__).parent.parent.parent))
from deep_learning.dl_config import (
    DL_FEATURES_DIR, DL_SEQUENCES_DIR,
    SEQUENCE_LENGTH, PREDICTION_HORIZONS, SEQUENCE_STRIDE, TRAIN_CONFIG
)


def load_dl_features():
    """åŠ è½½DLç‰¹å¾æ•°æ®"""
    features_file = DL_FEATURES_DIR / "dl_features.csv"
    if not features_file.exists():
        raise FileNotFoundError(f"DLç‰¹å¾æ–‡ä»¶ä¸å­˜åœ¨: {features_file}")

    df = pd.read_csv(features_file)
    df['datetime'] = pd.to_datetime(df['datetime'])
    return df


def create_sequences(df, seq_len, horizons, stride=1):
    """
    åˆ›å»ºæ»‘åŠ¨çª—å£åºåˆ—

    Args:
        df: ç‰¹å¾ DataFrameï¼ˆå« datetime å’Œ AQI åˆ—ï¼‰
        seq_len: è¾“å…¥åºåˆ—é•¿åº¦
        horizons: é¢„æµ‹æ—¶é—´è·¨åº¦åˆ—è¡¨
        stride: æ»‘åŠ¨æ­¥é•¿ï¼ˆé»˜è®¤1ï¼Œå¢å¤§å¯å‡å°‘æ ·æœ¬é‡å ï¼‰

    Returns:
        X: (N, seq_len, n_features) è¾“å…¥åºåˆ—
        y: (N, len(horizons)) ç›®æ ‡å€¼
        timestamps: æ¯ä¸ªæ ·æœ¬å¯¹åº”çš„æ—¶é—´æˆ³
    """
    feature_cols = [c for c in df.columns if c not in ['datetime', 'AQI']]
    target_col = 'AQI'

    if target_col not in df.columns:
        raise ValueError("æ•°æ®ä¸­æ²¡æœ‰ AQI åˆ—ä½œä¸ºç›®æ ‡å˜é‡")

    features = df[feature_cols].values
    targets = df[target_col].values
    timestamps = df['datetime'].values

    max_horizon = max(horizons)
    max_start = len(df) - seq_len - max_horizon

    if max_start <= 0:
        raise ValueError(f"æ•°æ®é‡ä¸è¶³: {len(df)} è¡Œ, éœ€è¦è‡³å°‘ {seq_len + max_horizon + 1} è¡Œ")

    indices = list(range(0, max_start, stride))
    n_samples = len(indices)

    X = np.zeros((n_samples, seq_len, len(feature_cols)), dtype=np.float32)
    y = np.zeros((n_samples, len(horizons)), dtype=np.float32)
    ts = []

    for idx, i in enumerate(indices):
        X[idx] = features[i:i + seq_len]
        for j, h in enumerate(horizons):
            y[idx, j] = targets[i + seq_len + h - 1]
        ts.append(timestamps[i + seq_len - 1])

    return X, y, np.array(ts), feature_cols


def split_data(X, y, timestamps, train_ratio, val_ratio):
    """
    æ—¶åºåˆ†å‰²ï¼ˆä¸æ‰“ä¹±é¡ºåºï¼‰

    Returns:
        dict: åŒ…å« train/val/test çš„ X, y, timestamps
    """
    n = len(X)
    train_end = int(n * train_ratio)
    val_end = int(n * (train_ratio + val_ratio))

    splits = {
        'train': {
            'X': X[:train_end],
            'y': y[:train_end],
            'timestamps': timestamps[:train_end],
        },
        'val': {
            'X': X[train_end:val_end],
            'y': y[train_end:val_end],
            'timestamps': timestamps[train_end:val_end],
        },
        'test': {
            'X': X[val_end:],
            'y': y[val_end:],
            'timestamps': timestamps[val_end:],
        },
    }

    return splits


def normalize_sequences(splits):
    """
    ä½¿ç”¨ StandardScaler å½’ä¸€åŒ–ï¼ˆä»… fit è®­ç»ƒé›†ï¼‰

    Args:
        splits: dict with train/val/test X arrays

    Returns:
        å½’ä¸€åŒ–åçš„ splits, scaler
    """
    train_X = splits['train']['X']
    n_train, seq_len, n_features = train_X.shape

    # å°†3D reshapeä¸º2Dæ¥fit scaler
    train_2d = train_X.reshape(-1, n_features)
    scaler = StandardScaler()
    scaler.fit(train_2d)

    # å¯¹æ¯ä¸ªsplitè¿›è¡Œå½’ä¸€åŒ–
    for key in ['train', 'val', 'test']:
        X = splits[key]['X']
        n, s, f = X.shape
        X_2d = X.reshape(-1, f)
        X_scaled = scaler.transform(X_2d)
        splits[key]['X'] = X_scaled.reshape(n, s, f).astype(np.float32)

    return splits, scaler


def build_sequences():
    """æ„å»ºå®Œæ•´çš„åºåˆ—æ•°æ®"""
    print("=" * 60)
    print("  åºåˆ—ç”Ÿæˆ")
    print("=" * 60)

    # åŠ è½½ç‰¹å¾
    print("\nğŸ“– åŠ è½½DLç‰¹å¾æ•°æ®...")
    df = load_dl_features()
    print(f"   æ•°æ®å½¢çŠ¶: {df.shape}")

    # åˆ›å»ºåºåˆ—
    print(f"\nğŸ”§ åˆ›å»ºæ»‘åŠ¨çª—å£åºåˆ— (çª—å£={SEQUENCE_LENGTH}h, æ­¥é•¿={SEQUENCE_STRIDE}, é¢„æµ‹={PREDICTION_HORIZONS})...")
    X, y, timestamps, feature_names = create_sequences(
        df, SEQUENCE_LENGTH, PREDICTION_HORIZONS, stride=SEQUENCE_STRIDE
    )
    print(f"   X shape: {X.shape}")
    print(f"   y shape: {y.shape}")
    print(f"   ç‰¹å¾æ•°: {len(feature_names)}")

    # æ—¶åºåˆ†å‰²
    train_ratio = TRAIN_CONFIG['train_ratio']
    val_ratio = TRAIN_CONFIG['val_ratio']
    print(f"\nğŸ“Š æ—¶åºåˆ†å‰² ({train_ratio}/{val_ratio}/{TRAIN_CONFIG['test_ratio']})...")
    splits = split_data(X, y, timestamps, train_ratio, val_ratio)

    for key in ['train', 'val', 'test']:
        print(f"   {key}: X={splits[key]['X'].shape}, y={splits[key]['y'].shape}")

    # å½’ä¸€åŒ–
    print("\nğŸ“ StandardScaler å½’ä¸€åŒ–ï¼ˆä»…fitè®­ç»ƒé›†ï¼‰...")
    splits, scaler = normalize_sequences(splits)

    # ä¿å­˜
    print(f"\nğŸ’¾ ä¿å­˜åˆ° {DL_SEQUENCES_DIR}...")

    for key in ['train', 'val', 'test']:
        np.save(DL_SEQUENCES_DIR / f"X_{key}.npy", splits[key]['X'])
        np.save(DL_SEQUENCES_DIR / f"y_{key}.npy", splits[key]['y'])
        np.save(DL_SEQUENCES_DIR / f"timestamps_{key}.npy", splits[key]['timestamps'])
        print(f"   {key}: X={splits[key]['X'].shape}, y={splits[key]['y'].shape}")

    # ä¿å­˜ scaler
    scaler_file = DL_SEQUENCES_DIR / "scaler.pkl"
    with open(scaler_file, 'wb') as f:
        pickle.dump(scaler, f)
    print(f"   scaler: {scaler_file}")

    # ä¿å­˜ç‰¹å¾å
    names_file = DL_SEQUENCES_DIR / "feature_names.txt"
    with open(names_file, 'w') as f:
        f.write('\n'.join(feature_names))
    print(f"   feature_names: {names_file}")

    # ä¿å­˜å…ƒæ•°æ®
    import json
    meta = {
        'sequence_length': SEQUENCE_LENGTH,
        'prediction_horizons': PREDICTION_HORIZONS,
        'n_features': len(feature_names),
        'n_train': len(splits['train']['X']),
        'n_val': len(splits['val']['X']),
        'n_test': len(splits['test']['X']),
        'feature_names': feature_names,
    }
    meta_file = DL_SEQUENCES_DIR / "metadata.json"
    with open(meta_file, 'w') as f:
        json.dump(meta, f, indent=2)

    print(f"\nâœ… åºåˆ—ç”Ÿæˆå®Œæˆ!")
    print(f"   æ€»æ ·æœ¬æ•°: {len(X)}")
    print(f"   N â‰ˆ {len(X)}")

    return splits, scaler, feature_names


def main():
    build_sequences()


if __name__ == "__main__":
    main()
